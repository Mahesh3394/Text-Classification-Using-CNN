{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDMgSstPYv0P"
   },
   "source": [
    "# Text Classification:\n",
    "\n",
    "## Data\n",
    "<pre>\n",
    "1. we have total of 20 types of documents(Text files) and total 18828 documents(text files).\n",
    "2. You can download data from this <a href='https://drive.google.com/open?id=1rxD15nyeIPIAZ-J2VYPrDRZI66-TBWvM'>link</a>, in that you will get documents.rar folder. <br>If you unzip that, you will get total of 18828 documnets. document name is defined as'ClassLabel_DocumentNumberInThatLabel'. \n",
    "so from document name, you can extract the label for that document.\n",
    "4. Now our problem is to classify all the documents into any one of the class.\n",
    "5. Below we provided count plot of all the labels in our data. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "64U9NzWFYv0V"
   },
   "outputs": [],
   "source": [
    "### count plot of all the class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-HWPLOWLhqX",
    "outputId": "390e8aed-5486-48e4-aa97-15ad490a09f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\scorpions3394\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\scorpions3394\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\scorpions3394\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\scorpions3394\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import regex as re\n",
    "import os\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input,Dense,Conv1D,Flatten,Embedding,MaxPool1D,concatenate,Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlqYFVI3Yv0k"
   },
   "source": [
    "#### sample document\n",
    "<pre>\n",
    "<font color='blue'>\n",
    "Subject: A word of advice\n",
    "From: jcopelan@nyx.cs.du.edu (The One and Only)\n",
    "\n",
    "In article < 65882@mimsy.umd.edu > mangoe@cs.umd.edu (Charley Wingate) writes:\n",
    ">\n",
    ">I've said 100 times that there is no \"alternative\" that should think you\n",
    ">might have caught on by now.  And there is no \"alternative\", but the point\n",
    ">is, \"rationality\" isn't an alternative either.  The problems of metaphysical\n",
    ">and religious knowledge are unsolvable-- or I should say, humans cannot\n",
    ">solve them.\n",
    "\n",
    "How does that saying go: Those who say it can't be done shouldn't interrupt\n",
    "those who are doing it.\n",
    "\n",
    "Jim\n",
    "--\n",
    "Have you washed your brain today?\n",
    "</font>\n",
    "</pre>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KAR5HoR1Yv0m"
   },
   "source": [
    "### Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FInd all emails in documents and make a list \n",
    "\n",
    "import re\n",
    "def email(doc):\n",
    "  final_mail = \"\"\n",
    "  match = re.findall(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', doc)\n",
    "  final_words = []\n",
    "  for i in match:\n",
    "    final = i.split('@')[1]\n",
    "    words = final.split('.')\n",
    "    for i in words:\n",
    "      if len(i)>2 and i != 'com':\n",
    "        final_words.append(i)\n",
    "  for i in final_words:\n",
    "      final_mail += \" \"+i\n",
    "  return final_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ktN4EAqSltwJ"
   },
   "outputs": [],
   "source": [
    "# 2. Replace all emails with space\n",
    "def replace_email(doc):\n",
    "  rep = re.sub(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+', ' ', doc)\n",
    "  return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6RGeN0TfyICP"
   },
   "outputs": [],
   "source": [
    "# 3. Get subject of the text i.e. get the total lines where \"Subject:\" occur and remove the word which are before the \":\" remove the newlines, tabs, punctuations, any special chars.\n",
    "\n",
    "def find_subject(doc):\n",
    "  if(re.findall(r'Subject:.*',doc)):\n",
    "    subject = re.findall(r'Subject:.*',doc)\n",
    "    subject = subject[0].split(\":\")[-1]\n",
    "    subject = re.sub('[^A-Za-z0-9]+',' ',subject)\n",
    "    subject = subject.lower()\n",
    "  return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "COq6Jfxb8Ht7"
   },
   "outputs": [],
   "source": [
    "# 4.After you store it in the list, Replace those sentances in original text by space.\n",
    "\n",
    "def rep_subject(doc):\n",
    "  rep_subject = re.sub(r'Subject:.*',' ',doc)\n",
    "  \n",
    "  return rep_subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w6B6zQKB81RF"
   },
   "outputs": [],
   "source": [
    "# 5.Delete all the sentances where sentence starts with \"Write to:\" or \"From:\".\n",
    "\n",
    "def rep_from(doc):\n",
    "  rep_writeto = re.sub(r'Write to:.*',' ',doc)\n",
    "  rep_from = re.sub(r'From:.*',' ',doc)\n",
    "  return rep_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "inNwSUYl-1fP"
   },
   "outputs": [],
   "source": [
    "# 6. Delete all the tags like \"< anyword >\"\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def remove_tag(doc):\n",
    "  soup = BeautifulSoup(doc,'lxml')\n",
    "  text = soup.get_text()\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NqtSUN8v_zyz"
   },
   "outputs": [],
   "source": [
    "#7. Delete all the data which are present in the brackets.\n",
    "\n",
    "def rep_bracket(doc):\n",
    "  string = re.sub(\"\\(.*?\\)\",\"\",doc)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cwwLBhZsAjgn"
   },
   "outputs": [],
   "source": [
    "#8. Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n",
    "\n",
    "def rep_new_line(doc):\n",
    "  string = re.sub(r\"\\s+\",\" \",doc)\n",
    "  string = re.sub(r\"[/]\",\".\",string)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G8BgK_iNA_wl"
   },
   "outputs": [],
   "source": [
    "#9. Remove all the words which ends with \":\".\n",
    "\n",
    "def rep_word(doc):\n",
    "  string = re.sub(r\"[a-zA-Z]+:\",\" \",doc)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WD_yq2ceCEGA"
   },
   "outputs": [],
   "source": [
    "#10. Decontractions, replace words like below to full words.\n",
    "\n",
    "def decontractions(doc):\n",
    "\n",
    "    string = re.sub(r\"can\\'t\",\"can not\",doc)\n",
    "    string = re.sub(r\"\\'s\",\"is\",string)\n",
    "    string = re.sub(r\"i\\'ve'\",\"i have\",string)\n",
    "    string = re.sub(r\"i\\'m\",\"i am\",string)\n",
    "    string = re.sub(r\"you\\'re\",\"you are\",string)\n",
    "    string = re.sub(r\"i\\'ll\",\"i will\",string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cW42TxoRFt7r"
   },
   "outputs": [],
   "source": [
    "# 11. Do chunking on the text you have after above preprocessing. \n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "def chunking(doc):\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(doc)),binary=True)\n",
    "        \n",
    "    for item in chunks:\n",
    "      if type(item)==Tree:\n",
    "        if item.label()=='PERSON':\n",
    "          for j,k in item.leaves():\n",
    "            text = re.sub(r'\\b{}\\b'.format(j),\" \",doc,count=1)\n",
    "                \n",
    "        else:\n",
    "          string = \"\"\n",
    "          chunked_string = \"\"\n",
    "            \n",
    "          for j,k in item.leaves():\n",
    "            string += j + \" \"\n",
    "            chunked_string += j + \"_\"\n",
    "                        \n",
    "                \n",
    "          string = string.strip()\n",
    "          chunked_string = chunked_string[:-1]\n",
    "                    \n",
    "          text = re.sub(r\"\\b{}\\b\".format(string),chunked_string,doc,count=1)\n",
    "                    \n",
    "    text = re.sub(r\"\\s+\",\" \",doc)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3CTXtmNzDFeF"
   },
   "outputs": [],
   "source": [
    "# 13. Replace all the digits with space i.e delete all the digits. \n",
    "def rep_digit(doc):\n",
    "  string = re.sub(r\"[0-9]\",\"\",doc)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-AfJtZNQDqQx"
   },
   "outputs": [],
   "source": [
    "#14.replace word with _word or word_\n",
    "def rep_word_(doc):\n",
    "  string = re.sub(r\"(_?)([A-Za-z0-9])(_?)\",r'\\2',doc)\n",
    "  return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1NCjowDzEdsN"
   },
   "outputs": [],
   "source": [
    "#15.We also observed some words like  \"OneLetter_word\"- eg: d_berlin,\n",
    "\n",
    "def OneLetter_word(doc):\n",
    "  string = re.sub(r\"([A-Za-z]{1,2})(_)(A-Za-z)\",\"\\g<3>\",doc)\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uc8Fz7hdE3yO"
   },
   "outputs": [],
   "source": [
    "#16. Convert all the words into lower case and lowe case and remove the words which are greater than or equal to 15 or less than or equal to 2.\n",
    "\n",
    "def lower_case(doc):\n",
    "  string =doc.lower()\n",
    "  string = ' '.join([w for w in string.split() if len(w)>2 and len(w)<15])\n",
    "\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "85lIMyoYFRXD"
   },
   "outputs": [],
   "source": [
    "# 17. replace all the words except \"A-Za-z_\" with space. \n",
    "\n",
    "def final_word(doc):\n",
    "  string = re.sub(r'[^A-Za-z_]',\" \",doc)\n",
    "\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "A9KyI0TMIGIR"
   },
   "outputs": [],
   "source": [
    "# finally consolidate all function to pre processing a document\n",
    "\n",
    "def preprocessing(filename):\n",
    "\n",
    "  class_ = filename.split('_')[0]           # finding a class label from filename\n",
    "\n",
    "  fhand = open('documents/'+filename, \"r\")\n",
    "  final_doc = fhand.read()                          # open a document using filename and path\n",
    "\n",
    "  emails = email(final_doc)      #------step 1 finding emails\n",
    "\n",
    "  subject = find_subject(final_doc)           #------------step 3 finding subject for each docuement\n",
    "\n",
    "  text = replace_email(final_doc) #-------step2\n",
    "\n",
    "  text = rep_subject(text)  #-------step4\n",
    "\n",
    "  text = rep_from(text)  #-------step5\n",
    "\n",
    "  text = remove_tag(text)  #-------step6\n",
    "\n",
    "  text = rep_bracket(text)  #-------step7\n",
    "\n",
    "  text = rep_new_line(text)  #-------step8\n",
    "\n",
    "  text = rep_word(text)  #-------step9\n",
    "\n",
    "  text = decontractions(text)  #-------step10\n",
    "\n",
    "  text = chunking(text)  #-------step11\n",
    "\n",
    "  text = rep_digit(text)  #-------step13\n",
    "\n",
    "  text = rep_word_(text)  #-------step14\n",
    "\n",
    "  text = lower_case(text)  #-------step16\n",
    "\n",
    "  text = final_word(text)  #-------step17\n",
    "\n",
    "  return [class_,emails,subject,text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OxljvYh_K2z-"
   },
   "outputs": [],
   "source": [
    "# lets check it is working fine\n",
    "lst = preprocessing('alt.atheism_51125.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdyk4TnAPhCW",
    "outputId": "cd920ccd-e179-4495-cee6-28fc435ef3ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " ' cco caltech edu solntze wpd sgi',\n",
       " ' political atheists ',\n",
       " '       there good deal more confusion here  you started off with the  assertion that there was some  objective  morality  and you admit  here  you finished with recursive definition  murder   objectively  immoral  but eactly what murder and what not itself  requires appeal morality  yes   now you have switch targets little  but only little  now you are  asking what the  goal   what you mean  goal    are you  suggesting that there some  objective   goal  out there somewhere   and form our morals achieve it  well  for example  the goal  natural  morality the survival and propogation the species  another example moral system presented within the declaration independence  which states that should guaranteed life liberty and the pursuit happiness  you see  have moral system  must define the purpose the system  that is  shall moral unto what end    murder certainly violation the golden rule  and  thought had   defined murder intentional killing non murderer  against his will    and you responded this asking whether not the execution   innocent person under our system capital punishment was murder not    i fail see what this has with anything  never claimed that our   system morality was objective one  thought that was your very first claim  that there was  some kind  objective  morality  and that example that was  that murder wrong  you don t want claim that any more   thatis fine  well  murder violates the golen rule  which certainly pillar most every moral system  however  not assuming that our current system and the manner its implementation are objectively moral  think that very good approximation  but can not perfect   and the way  you don t seem understand the difference between   arbitrary  and  objective   keith schneider  defines  murder  to this that and the other  thatis arbitrary  jon livesey may  still say  well  according personal system morality  all  killing humans against their will murder  and wrong  and what  the legal definition murder may the usa  kuweit  saudi  arabia  the prc may matters not whit me   well   objective  would assume system based clear and fundamental concepts  while  arbitary  implies clear line reasoning  keith']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess each document in folder\n",
    "\n",
    "row = []\n",
    "for f in os.listdir('documents'):\n",
    "    \n",
    "    lst = preprocessing(f)\n",
    "    row.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " ' cco caltech edu jyusenkyou jhu edu',\n",
       " ' keith schneider stealth poster ',\n",
       " '  but  you were discuss the merits racism  its psycholgical   benefits  you would well have experienced personally   when you speak  experiencing religion  you mean someone should believe religion  thatis right  and this pretty impossible  right  would ideal could believe for while  just try out religion  and only then determine which course thought suits best  but again  this not possible  not that religion warrants belief  but the belief carries with some psychological benefits  there are also some psychological burdens  too   when you speak  experiencing racism   you mean that someone should  believe racism  that they should have racist things done them  for  parallelism  the former must what you meant  but seems odd  usage the phrase  well  there were some psychological other benefits gained from racism  they could only fully understood judged persons actually  believing  racism  course  the parallel happens poor one  but you originated it  keith']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have created a list \n",
    "row[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe using pandas dataframe\n",
    "import pandas as pd\n",
    "final_data = pd.DataFrame(row,columns = [\"class\",\"email\",\"subject\",\"preprocessed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hB43OGEfYv1C"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>email</th>\n",
       "      <th>subject</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu solntze wpd sgi</td>\n",
       "      <td>political atheists</td>\n",
       "      <td>there good deal more confusion here  yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu CWRU edu</td>\n",
       "      <td>pompous ass</td>\n",
       "      <td>then why people keep asking the same questio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                             email               subject  \\\n",
       "0  alt.atheism   cco caltech edu solntze wpd sgi   political atheists    \n",
       "1  alt.atheism          cco caltech edu CWRU edu           pompous ass   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0         there good deal more confusion here  yo...  \n",
       "1    then why people keep asking the same questio...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_pickle(\"./preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18820, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n3ucJLtWYv1V"
   },
   "source": [
    "### Training The models to Classify: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>email</th>\n",
       "      <th>subject</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu solntze wpd sgi</td>\n",
       "      <td>political atheists</td>\n",
       "      <td>there good deal more confusion here  yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu CWRU edu</td>\n",
       "      <td>pompous ass</td>\n",
       "      <td>then why people keep asking the same questio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                             email               subject  \\\n",
       "0  alt.atheism   cco caltech edu solntze wpd sgi   political atheists    \n",
       "1  alt.atheism          cco caltech edu CWRU edu           pompous ass   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0         there good deal more confusion here  yo...  \n",
       "1    then why people keep asking the same questio...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column with consolidated data\n",
    "final_data['total_data'] = final_data[['email','subject','preprocessed_text']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>email</th>\n",
       "      <th>subject</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>total_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu solntze wpd sgi</td>\n",
       "      <td>political atheists</td>\n",
       "      <td>there good deal more confusion here  yo...</td>\n",
       "      <td>cco caltech edu solntze wpd sgi  political at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>cco caltech edu CWRU edu</td>\n",
       "      <td>pompous ass</td>\n",
       "      <td>then why people keep asking the same questio...</td>\n",
       "      <td>cco caltech edu CWRU edu  pompous ass   then ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                             email               subject  \\\n",
       "0  alt.atheism   cco caltech edu solntze wpd sgi   political atheists    \n",
       "1  alt.atheism          cco caltech edu CWRU edu           pompous ass   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0         there good deal more confusion here  yo...   \n",
       "1    then why people keep asking the same questio...   \n",
       "\n",
       "                                          total_data  \n",
       "0   cco caltech edu solntze wpd sgi  political at...  \n",
       "1   cco caltech edu CWRU edu  pompous ass   then ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if it is fine\n",
    "final_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find X and Y values\n",
    "final_model_data = final_data[['class','total_data']]\n",
    "x = final_data['total_data']\n",
    "y = final_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18820, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "## encoding lables  Encode target labels with value between 0 and n_classes-1.\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y_encoder = encoder.transform(y)\n",
    "\n",
    "#np_utils.to_categorical is used to convert array of labeled data(from 0 to nb_classes - 1) to one-hot vector.\n",
    "y = np_utils.to_categorical(y_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data in 75% and 25 %\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a tokenizer \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train  = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train,maxlen=5000,padding=\"post\")\n",
    "x_test= pad_sequences(x_test,maxlen=5000,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "pretrain =  open(\"glove.6B.50d.txt\",encoding=\"utf8\") \n",
    "for i in pretrain:\n",
    "    value = i.split(\" \")\n",
    "    word = value[0]\n",
    "    vector = np.asarray(value[1:])\n",
    "    embedding_dict[word] = vector\n",
    "pretrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80109"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting embedding word dictionary to embedding matrix \n",
    "\n",
    "emb_matrix = np.zeros((len(tokenizer.index_word)+1,50))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    emb_word = embedding_dict.get(word)\n",
    "\n",
    "    if emb_word is not None:\n",
    "      emb_matrix[i]=emb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80110, 50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding layer \n",
    "embedding_layer = Embedding(len(tokenizer.word_index)+1, 50, embeddings_initializer=tf.keras.initializers.Constant(emb_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_0 = Input(shape=(5000))\n",
    "\n",
    "embed_layer = embedding_layer(layer_0)\n",
    "\n",
    "l_1 = Conv1D(16,4,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed_layer)\n",
    "\n",
    "l_2 = Conv1D(16,4,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed_layer)\n",
    "\n",
    "l_3 = Conv1D(16,4,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(embed_layer)\n",
    "\n",
    "second_layer = concatenate([l_1,l_2,l_3])\n",
    "\n",
    "max_pool_1 = MaxPool1D(3)(second_layer)\n",
    "\n",
    "l_4 = Conv1D(16,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "\n",
    "l_5 = Conv1D(16,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "\n",
    "l_6 =  Conv1D(16,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_1)\n",
    "\n",
    "third_layer = concatenate([l_4,l_5,l_6])\n",
    "\n",
    "max_pool_2 = MaxPool1D(3)(third_layer)\n",
    "\n",
    "fourth_layer = Conv1D(16,3,activation='relu',\n",
    "                      kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l2())(max_pool_2)\n",
    "\n",
    "flatten = Flatten()(fourth_layer)\n",
    "\n",
    "dropout_layer = Dropout(0.2)(flatten)\n",
    "\n",
    "dense_layer = Dense(32,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal())(dropout_layer)\n",
    "              \n",
    "output_layer = Dense(20,activation=\"softmax\",kernel_initializer= tf.keras.initializers.glorot_normal())(dense_layer)\n",
    "              \n",
    "\n",
    "cnn_model =Model(inputs=layer_0,outputs=output_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 5000, 50)     4005500     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 4997, 16)     3216        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 4997, 16)     3216        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 4997, 16)     3216        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4997, 48)     0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1665, 48)     0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 1663, 16)     2320        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 1663, 16)     2320        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 1663, 16)     2320        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1663, 48)     0           ['conv1d_3[0][0]',               \n",
      "                                                                  'conv1d_4[0][0]',               \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 554, 48)     0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 552, 16)      2320        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 8832)         0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 8832)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           282656      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 20)           660         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,307,744\n",
      "Trainable params: 4,307,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22359, saving model to best_model_1.h5\n",
      "221/221 - 185s - loss: 3.5135 - accuracy: 0.1246 - f1_score: 0.1246 - val_loss: 2.5800 - val_accuracy: 0.2236 - val_f1_score: 0.2236 - 185s/epoch - 837ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.22359 to 0.45845, saving model to best_model_1.h5\n",
      "221/221 - 179s - loss: 2.1909 - accuracy: 0.3365 - f1_score: 0.3365 - val_loss: 1.8420 - val_accuracy: 0.4584 - val_f1_score: 0.4584 - 179s/epoch - 811ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.45845 to 0.55622, saving model to best_model_1.h5\n",
      "221/221 - 179s - loss: 1.5833 - accuracy: 0.5408 - f1_score: 0.5408 - val_loss: 1.5347 - val_accuracy: 0.5562 - val_f1_score: 0.5562 - 179s/epoch - 809ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.55622 to 0.62869, saving model to best_model_1.h5\n",
      "221/221 - 177s - loss: 1.2236 - accuracy: 0.6673 - f1_score: 0.6673 - val_loss: 1.3654 - val_accuracy: 0.6287 - val_f1_score: 0.6287 - 177s/epoch - 800ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.62869 to 0.70032, saving model to best_model_1.h5\n",
      "221/221 - 176s - loss: 0.9642 - accuracy: 0.7640 - f1_score: 0.7640 - val_loss: 1.1933 - val_accuracy: 0.7003 - val_f1_score: 0.7003 - 176s/epoch - 798ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.70032 to 0.71647, saving model to best_model_1.h5\n",
      "221/221 - 176s - loss: 0.7990 - accuracy: 0.8317 - f1_score: 0.8317 - val_loss: 1.1742 - val_accuracy: 0.7165 - val_f1_score: 0.7165 - 176s/epoch - 797ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.71647 to 0.73773, saving model to best_model_1.h5\n",
      "221/221 - 180s - loss: 0.6263 - accuracy: 0.8877 - f1_score: 0.8877 - val_loss: 1.1557 - val_accuracy: 0.7377 - val_f1_score: 0.7377 - 180s/epoch - 815ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.73773 to 0.75473, saving model to best_model_1.h5\n",
      "221/221 - 181s - loss: 0.5352 - accuracy: 0.9177 - f1_score: 0.9177 - val_loss: 1.1571 - val_accuracy: 0.7547 - val_f1_score: 0.7547 - 181s/epoch - 817ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.75473\n",
      "221/221 - 186s - loss: 0.4656 - accuracy: 0.9392 - f1_score: 0.9392 - val_loss: 1.1893 - val_accuracy: 0.7430 - val_f1_score: 0.7430 - 186s/epoch - 843ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.75473 to 0.76812, saving model to best_model_1.h5\n",
      "221/221 - 188s - loss: 0.4036 - accuracy: 0.9567 - f1_score: 0.9567 - val_loss: 1.1414 - val_accuracy: 0.7681 - val_f1_score: 0.7681 - 188s/epoch - 852ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21dd3c287c0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_1.h5',verbose=1,monitor='val_accuracy',\n",
    "                           mode='auto',save_best_only=True)\n",
    "\n",
    "## Tensorboard\n",
    "log_dir = \"logs\"\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "## compile model \n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy',F1Score(average='micro',num_classes=20)])\n",
    "\n",
    "## Callback for earlystopping \n",
    "early_stop = EarlyStopping(monitor=\"val_accuracy\",mode='max',patience=2,verbose=1)\n",
    "\n",
    "## Trainning \n",
    "\n",
    "\n",
    "cnn_model.fit(x_train,y_train,epochs=10,verbose=2,validation_data=(x_test,y_test),batch_size =64,callbacks=[checkpoint,tensorboard,early_stop]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(cnn_model,to_file = 'model1.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain data with model 2\n",
    "# train and test split again\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tokenizer with char level true\n",
    "# this use character as tokens\n",
    "\n",
    "tok_char = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',char_level= True,oov_token='UNK')\n",
    "tok_char.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize train and test data using tokenizer\n",
    "x_train = tok_char.texts_to_sequences(x_train)\n",
    "x_test = tok_char.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequence to same size\n",
    "\n",
    "x_train = pad_sequences(x_train,maxlen=5000,padding=\"post\")\n",
    "x_test = pad_sequences(x_test,maxlen=5000,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a embedding matrix \n",
    "\n",
    "size = len(tok_char.word_index)+1\n",
    "\n",
    "matrix_char = np.zeros((size,size))\n",
    "\n",
    "for i,j in tok_char.word_index.items():\n",
    "  matrix_char[j][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding layer with characters\n",
    "\n",
    "embedding_layer_char = Embedding(len(tok_char.word_index)+1,41, embeddings_initializer=tf.keras.initializers.Constant(matrix_char),input_length=5000,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "layer_0 = Input(shape=(5000))\n",
    "\n",
    "embed = embedding_layer_char(layer_0)\n",
    "\n",
    "\n",
    "l_1 = Conv1D(64,3,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(embed)\n",
    "\n",
    "l_2 = Conv1D(64,3,activation=\"relu\",kernel_initializer =tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(l_1)\n",
    "\n",
    "\n",
    "max_pool_1 = MaxPool1D(5)(l_2)\n",
    "\n",
    "l_3 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(max_pool_1)\n",
    "\n",
    "l_4 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(l_3)\n",
    "\n",
    "\n",
    "max_pool_2 = MaxPool1D(5)(l_4)\n",
    "\n",
    "l_5 = Conv1D(64,3,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42),kernel_regularizer=tf.keras.regularizers.l1())(max_pool_2)\n",
    "\n",
    "\n",
    "max_pool_3 = MaxPool1D(5)(l_5)\n",
    "\n",
    "flatten = Flatten()(max_pool_3)\n",
    "\n",
    "dropout_layer = Dropout(0.5)(flatten)\n",
    "\n",
    "dense_layer1 = Dense(128,activation=\"relu\",kernel_initializer = tf.keras.initializers.he_normal(seed=42))(dropout_layer)\n",
    "\n",
    "\n",
    "              \n",
    "output_layer = Dense(20,activation=\"softmax\",kernel_initializer= tf.keras.initializers.glorot_normal(seed=42))(dense_layer1)\n",
    "              \n",
    "\n",
    "model_char =Model(inputs=layer_0,outputs=output_layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 5000)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 5000, 41)          1681      \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 4998, 64)          7936      \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 4996, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 999, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 997, 64)           12352     \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 995, 64)           12352     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 199, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 197, 64)           12352     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 39, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2496)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2496)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               319616    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 381,221\n",
      "Trainable params: 379,540\n",
      "Non-trainable params: 1,681\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "\n",
    "model_char.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.05250, saving model to best_model_2.h5\n",
      "221/221 - 134s - loss: 3.0631 - accuracy: 0.0504 - f1_score: 0.0504 - val_loss: 3.0637 - val_accuracy: 0.0525 - val_f1_score: 0.0525 - 134s/epoch - 609ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.05250 to 0.05271, saving model to best_model_2.h5\n",
      "221/221 - 135s - loss: 3.0523 - accuracy: 0.0493 - f1_score: 0.0493 - val_loss: 3.0487 - val_accuracy: 0.0527 - val_f1_score: 0.0527 - 135s/epoch - 612ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.05271 to 0.05292, saving model to best_model_2.h5\n",
      "221/221 - 142s - loss: 3.0523 - accuracy: 0.0499 - f1_score: 0.0499 - val_loss: 3.0491 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 142s/epoch - 642ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.05292\n",
      "221/221 - 143s - loss: 3.0526 - accuracy: 0.0467 - f1_score: 0.0467 - val_loss: 3.0640 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 143s/epoch - 645ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.05292\n",
      "221/221 - 141s - loss: 3.0523 - accuracy: 0.0502 - f1_score: 0.0502 - val_loss: 3.0392 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 141s/epoch - 637ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.05292 to 0.05313, saving model to best_model_2.h5\n",
      "221/221 - 141s - loss: 3.0521 - accuracy: 0.0523 - f1_score: 0.0523 - val_loss: 3.0439 - val_accuracy: 0.0531 - val_f1_score: 0.0531 - 141s/epoch - 636ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.05313\n",
      "221/221 - 139s - loss: 3.0521 - accuracy: 0.0481 - f1_score: 0.0481 - val_loss: 3.0423 - val_accuracy: 0.0531 - val_f1_score: 0.0531 - 139s/epoch - 628ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.05313\n",
      "221/221 - 139s - loss: 3.0524 - accuracy: 0.0493 - f1_score: 0.0493 - val_loss: 3.0497 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 139s/epoch - 630ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.05313\n",
      "221/221 - 140s - loss: 3.0521 - accuracy: 0.0494 - f1_score: 0.0494 - val_loss: 3.0471 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 140s/epoch - 632ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.05313\n",
      "221/221 - 139s - loss: 3.0520 - accuracy: 0.0502 - f1_score: 0.0502 - val_loss: 3.0569 - val_accuracy: 0.0529 - val_f1_score: 0.0529 - 139s/epoch - 631ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21df05643d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling and training model\n",
    "\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_model_2.h5',verbose=1,monitor='val_accuracy',\n",
    "                           mode='auto',save_best_only=True)\n",
    "\n",
    "## Tensorboard\n",
    "log_dir = \"logs\"\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "\n",
    "## compile model \n",
    "model_char.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy',F1Score(average='micro',num_classes=20)])\n",
    "\n",
    "\n",
    "\n",
    "## Trainning \n",
    "\n",
    "\n",
    "model_char.fit(x_train,y_train,epochs=10,verbose=2,validation_data=(x_test,y_test),batch_size =64,callbacks=[checkpoint,tensorboard]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0mwdtcvYv1X"
   },
   "source": [
    "### Model-1: Using 1D convolutions with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXPPsovJ3ePk"
   },
   "source": [
    "<pre>\n",
    "<b>Encoding of the Text </b> --> For a given text data create a Matrix with Embedding layer as shown Below. \n",
    "In the example we have considered d = 5, but in this assignment we will get d = dimension of Word vectors we are using.\n",
    " i.e if we have maximum of 350 words in a sentence and embedding of 300 dim word vector, \n",
    " we result in 350*300 dimensional matrix for each sentance as output after embedding layer\n",
    "<img src='https://i.imgur.com/kiVQuk1.png'>\n",
    "Ref: https://i.imgur.com/kiVQuk1.png\n",
    "\n",
    "<b>Reference:</b>\n",
    "<a href='https://stackoverflow.com/a/43399308/4084039'>https://stackoverflow.com/a/43399308/4084039</a>\n",
    "<a href='https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/'>https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/</a>\n",
    "\n",
    "<b><a href='https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work'>How EMBEDDING LAYER WORKS </a></b>\n",
    "\n",
    "</pre>\n",
    "\n",
    "### Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGVQKge3Yv1e"
   },
   "source": [
    "<img src='https://i.imgur.com/fv1GvFJ.png'>\n",
    "ref: 'https://i.imgur.com/fv1GvFJ.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC6SBG5AYv1f"
   },
   "source": [
    "<pre>\n",
    "1. all are Conv1D layers with any number of filter and filter sizes, there is no restriction on this.\n",
    "\n",
    "2. use concatenate layer is to concatenate all the filters/channels. \n",
    "\n",
    "3. You can use any pool size and stride for maxpooling layer.\n",
    "\n",
    "4. Don't use more than 16 filters in one Conv layer becuase it will increase the no of params. \n",
    "( Only recommendation if you have less computing power )\n",
    "\n",
    "5. You can use any number of layers after the Flatten Layer.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cg4L1V4Yv1d"
   },
   "source": [
    "### Model-2 : Using 1D convolutions with character embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Djg4YVA3oQx"
   },
   "source": [
    "<pre>\n",
    "<pre><img src=\"https://i.ytimg.com/vi/CNY8VjJt-iQ/maxresdefault.jpg\" width=\"70%\">\n",
    "Here are the some papers based on Char-CNN\n",
    " 1. Xiang Zhang, Junbo Zhao, Yann LeCun. <a href=\"http://arxiv.org/abs/1509.01626\">Character-level Convolutional Networks for Text Classification</a>.NIPS 2015\n",
    " 2. Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. <a href=\"https://arxiv.org/abs/1508.06615\">Character-Aware Neural Language Models</a>. AAAI 2016\n",
    " 3. Shaojie Bai, J. Zico Kolter, Vladlen Koltun. <a href=\"https://arxiv.org/pdf/1803.01271.pdf\">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</a>\n",
    " 4. Use the pratrained char embeddings <a href='https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt'>https://github.com/minimaxir/char-embeddings/blob/master/glove.840B.300d-char.txt</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXvKSEIeSvN5"
   },
   "source": [
    "<img src='https://i.imgur.com/EuuoJtr.png'>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
